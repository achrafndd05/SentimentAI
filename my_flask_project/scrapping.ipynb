{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_data(card):\n",
    "    \"\"\"Extract data from tweet card\"\"\"\n",
    "    username = card.find_element(By.XPATH,'.//span').text\n",
    "    try:\n",
    "        handle = card.find_element(By.XPATH,'.//span[contains(text(), \"@\")]').text\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        postdate = card.find_element(By.XPATH,'.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    comment = card.find_element(By.XPATH,'/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[3]/div/section/div/div/div[1]/div/div/article/div/div/div[2]/div[2]/div[2]/div').text\n",
    "    text = comment\n",
    "\n",
    "\n",
    "    \n",
    "    tweet = text \n",
    "    return tweet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "hach = '#'+'football'\n",
    "# create instance of web driver\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# navigate to login screen\n",
    "driver.get('https://twitter.com/i/flow/login?input_flow_data=%7B%22requested_variant%22%3A%22eyJsYW5nIjoiZnIifQ%3D%3D%22%7D')\n",
    "driver.maximize_window()\n",
    "sleep(5)\n",
    "\n",
    "# find search input and search for term\n",
    "search_input_mail = driver.find_element(By.XPATH,'//input[@name=\"text\"]')\n",
    "search_input_mail.send_keys('mn.nedjah@gmail.com')\n",
    "search_input_mail.send_keys(Keys.RETURN)\n",
    "sleep(1)\n",
    "#password\n",
    "try:\n",
    "    search_input_password = driver.find_element(By.XPATH,'//input[@name=\"password\"]')\n",
    "    search_input_password.send_keys('mahmoud2030')\n",
    "    search_input_password.send_keys(Keys.RETURN)\n",
    "    sleep(1)\n",
    "except:\n",
    "    # find search input and search for term\n",
    "    search_input_us = driver.find_element(By.XPATH,'//input[@name=\"text\"]')\n",
    "    search_input_us.send_keys('MnNazih')\n",
    "    search_input_us.send_keys(Keys.RETURN)\n",
    "    sleep(1)\n",
    "    search_input_password = driver.find_element(By.XPATH,'//input[@name=\"password\"]')\n",
    "    search_input_password.send_keys('mahmoud2030')\n",
    "    search_input_password.send_keys(Keys.RETURN)\n",
    "    sleep(1)\n",
    "\n",
    "# find search input and search for term\n",
    "sleep(5)\n",
    "search_input = driver.find_element(By.XPATH,'//input[@aria-label=\"Search query\"]')\n",
    "search_input.send_keys(hach)\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "sleep(5)\n",
    "# Locate the element by link text and click on it\n",
    "element = driver.find_element(By.LINK_TEXT, 'Latest')\n",
    "element.click()\n",
    "scroll_attempt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "num = 3\n",
    "data = []\n",
    "tweet_ids = set()\n",
    "last_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "scroll_attempt = 0\n",
    "scrolling = True\n",
    "sleep(5)\n",
    "\n",
    "while scrolling:\n",
    "    page_cards = driver.find_elements(By.XPATH, '//article[@data-testid=\"tweet\"]')\n",
    "    for card in page_cards[-15:]:\n",
    "        \n",
    "        tweet = get_tweet_data(card)  # Replace with appropriate code to extract tweet data\n",
    "        tweet_id = ''.join(tweet)\n",
    "        data.append(tweet)\n",
    "\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "\n",
    "    scroll_attempt += 1\n",
    "\n",
    "    if scroll_attempt >= num:\n",
    "        curr_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        scrolling = False\n",
    "    else:\n",
    "        sleep(2)  # Attempt another scroll\n",
    "\n",
    "# Close the web driver\n",
    "#driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prime hazard #Hazard #football #life', 'prime hazard #Hazard #football #life', 'prime hazard #Hazard #football #life', 'Bonjour a tous, Je me m appelle Coulibaly Samuel, j ai 17ans, je suis originaire de Côte d Ivoire et j ai toujours rêvé de devenir footballeur professionnel. #football #DONS', 'Bonjour a tous, Je me m appelle Coulibaly Samuel, j ai 17ans, je suis originaire de Côte d Ivoire et j ai toujours rêvé de devenir footballeur professionnel. #football #DONS', 'Bonjour a tous, Je me m appelle Coulibaly Samuel, j ai 17ans, je suis originaire de Côte d Ivoire et j ai toujours rêvé de devenir footballeur professionnel. #football #DONS', 'Bonjour a tous, Je me m appelle Coulibaly Samuel, j ai 17ans, je suis originaire de Côte d Ivoire et j ai toujours rêvé de devenir footballeur professionnel. #football #DONS', 'Bonjour a tous, Je me m appelle Coulibaly Samuel, j ai 17ans, je suis originaire de Côte d Ivoire et j ai toujours rêvé de devenir footballeur professionnel. #football #DONS', 'Bonjour a tous, Je me m appelle Coulibaly Samuel, j ai 17ans, je suis originaire de Côte d Ivoire et j ai toujours rêvé de devenir footballeur professionnel. #football #DONS', 'Day 1 / Post 27 -- $4/each\\n\\n1 is /299 acetate\\n2 is /1952\\n4 is /25\\n\\nClaim by number. See pinned for shipping. Sale Ends 6/13! #JuneiqueFindsStacks\\n@SleepyCards_RT\\n @sports_sell\\n@UniqueFindsRTs\\n #Football #NFL', 'Day 1 / Post 27 -- $4/each\\n\\n1 is /299 acetate\\n2 is /1952\\n4 is /25\\n\\nClaim by number. See pinned for shipping. Sale Ends 6/13! #JuneiqueFindsStacks\\n@SleepyCards_RT\\n @sports_sell\\n@UniqueFindsRTs\\n #Football #NFL', 'Day 1 / Post 27 -- $4/each\\n\\n1 is /299 acetate\\n2 is /1952\\n4 is /25\\n\\nClaim by number. See pinned for shipping. Sale Ends 6/13! #JuneiqueFindsStacks\\n@SleepyCards_RT\\n @sports_sell\\n@UniqueFindsRTs\\n #Football #NFL', 'Day 1 / Post 27 -- $4/each\\n\\n1 is /299 acetate\\n2 is /1952\\n4 is /25\\n\\nClaim by number. See pinned for shipping. Sale Ends 6/13! #JuneiqueFindsStacks\\n@SleepyCards_RT\\n @sports_sell\\n@UniqueFindsRTs\\n #Football #NFL', 'Day 1 / Post 27 -- $4/each\\n\\n1 is /299 acetate\\n2 is /1952\\n4 is /25\\n\\nClaim by number. See pinned for shipping. Sale Ends 6/13! #JuneiqueFindsStacks\\n@SleepyCards_RT\\n @sports_sell\\n@UniqueFindsRTs\\n #Football #NFL', 'Day 1 / Post 27 -- $4/each\\n\\n1 is /299 acetate\\n2 is /1952\\n4 is /25\\n\\nClaim by number. See pinned for shipping. Sale Ends 6/13! #JuneiqueFindsStacks\\n@SleepyCards_RT\\n @sports_sell\\n@UniqueFindsRTs\\n #Football #NFL']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(len(data))\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates()\n",
    "with open('turkcell_tweets.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    header = ['Text']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0               prime hazard #Hazard #football #life\n",
      "3  Bonjour a tous, Je me m appelle Coulibaly Samu...\n",
      "9  Day 1 / Post 27 -- $4/each\\n\\n1 is /299 acetat...\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a pandas DataFrame called 'df'\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Alternatively, you can specify columns to check for duplicates\n",
    "# df = df.drop_duplicates(subset=['column1', 'column2'])\n",
    "\n",
    "# Print the DataFrame after removing duplicates\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
